# Behavioural analysis script
rm(list=ls())

cd<-getwd()


library(ggplot2)
library(dplyr)
library(psycho)
library(lme4)
library(car)
library(sjstats)
library(brms)

source("helper_functions/discCalc.R")

setwd("premup-pilot")

# in this dataset, there are 313 old items. by 30 participants, there are 9360 trials. 

Datalong<-read.csv2("outputs/group_level/share/group_task-rec.csv",sep=";",
                    header=T,stringsAsFactors=FALSE)

Datalong$rec_trialType<-Datalong$rec_trial_type

# calculate discriminability by participant
# first, create wide dataset
Datawide<-discCalc(Datalong)

# histogram dprim
hist(Datawide$dprime)

# participants who got less than 1+2SD
outcut<-mean(Datawide$dprime)

excl<-Datawide$SubNum[Datawide$dprime<outcut-2* sd(Datawide$dprime)]

# select only old trials
Datalong<-Datalong[(Datalong$OvsN==1),]

# exclude fillers
Datalong<-Datalong[Datalong$fillers==0,]

# count by trials
countTrials<-Datalong %>%
  group_by(PE_level) %>%
  tally()

# divide by participants
countTrials$n<-countTrials$n/length(unique(Datalong$participant))

countTrials

# exclude participant
Datalong<-Datalong[!Datalong$participant %in% excl, ]
      
# aggregate data by session, condition, participants
data_agg<-Datalong %>%
  group_by(session, PE_level, participant) %>%
  dplyr::summarise(id_acc = mean(id_acc, na.rm = T))

# congert the factors variables
data_agg$PE_level<-as.factor(data_agg$PE_level)
data_agg$session<-as.factor(data_agg$session)

# rename the levels
levels(data_agg$PE_level)<-c("LowPE", "MediumPE", "HighPE")
levels(data_agg$session)<-c("ImmediateRec", "DelayedRec")

# get within participant SE
library(Rmisc)
dat_summary <- summarySEwithin(data_agg,
                                measurevar = "id_acc",
                                withinvars = c("PE_level", "session"), 
                                idvar = "participant")




ggplot(data_agg, aes(PE_level, id_acc))+ 
  geom_bar(aes(PE_level, id_acc, fill = PE_level),
       position="dodge",stat="summary", fun.y="mean")+
 
  geom_errorbar(aes(y = id_acc, ymin = id_acc - ci, ymax = id_acc + ci),
              color = "black", width = 0.10, data=dat_summary)+
  #geom_boxplot(alpha =0.1)+
  geom_jitter( size=0.4,width=0.1, data=data_agg)+
  theme_bw()+
  theme(axis.text.x = element_blank())+ # we are showing the different levels through the colors, so we can avoid naming the bars
  theme(axis.ticks.x = element_blank())+
  facet_grid(.~session)+
  theme(strip.text.x = element_text(size = 13))+ 
  ylim(c(0,1))+
  ylab("% recognition")

# unconditional model first to check the intraclass correlation
unconditionalM<-glmer(id_acc~(1|participant), family=binomial(), data= Datalong)
summary(unconditionalM)

# intraclass correlation
ic1<-performance::icc(unconditionalM)
# 15% of the variance in recognition memory is between participants

# does adding random intercepts for images improve the fit?
unconditionalMandIM<-glmer(id_acc~(1|participant)+(1|rec_Obj), family=binomial(), data= Datalong)
summary(unconditionalMandIM)

anova(unconditionalM, unconditionalMandIM)
# significant improvement

# does adding random slopes for PE improves it?
# first, convert PE level into factor
Datalong$PE_level<-as.factor(Datalong$PE_level)
unconditionalMimandSesPE<-glmer(id_acc~(1+PE_level|participant)+(1|rec_Obj), family=binomial(), data= Datalong,
                                control=glmerControl(optimizer="bobyqa",optCtrl=list(maxfun=100000)))
# singular fit

anova(unconditionalMandIM, unconditionalMimandSesPE)
# does not improve the fit




# now maximal random structure
modelmaximal<-glmer(id_acc~PE_level*session+(1+session*PE_level|participant)+(1|rec_Obj), 
                    family=binomial(), data= Datalong,
                    control=glmerControl(optimizer="bobyqa",optCtrl=list(maxfun=100000)))

modelmaximal<-glmer(id_acc~PE_level*session+(1|participant/session:PE_level), 
                    family=binomial(), data= Datalong,
                    control=glmerControl(optimizer="bobyqa",optCtrl=list(maxfun=100000)))

modelmaximal<-glmer(id_acc~PE_level*session+(1|participant/session:PE_level),
                    #(1|participant/session:PE_level), 
                    family=binomial(), data= Datalong,
                    control=glmerControl(optimizer="bobyqa",optCtrl=list(maxfun=100000)))



summary(modelmaximal)

Anova(modelmaximal , type="II")

# try uncorrelated random intercepts and slopws
# now maximal random structure, uncorrelated intercepts and slopes
modelmaximal<-glmer(id_acc~PE_level*session+(0+session*PE_level|participant)+(1|rec_Obj), 
                    family=binomial(), data= Datalong,
                    control=glmerControl(optimizer="bobyqa",optCtrl=list(maxfun=100000)))

summary(modelmaximal)

Anova(modelmaximal, type = "III")

# try to take only 48 trials for PE medium, in order to have balanced structure
# create count for trials medium PE
Datalong$countTR<-NA
countr<-0
for (n in 1:nrow(Datalong)){
  
  if (Datalong$PE_level[n]=="0.67"){
    if (countr<72){
    Datalong$countTR[n]<-countr+1
    countr<-countr+1
    } else{
      Datalong$countTR[n]<-72
      countr<-0
  }
  }
}

DatalongBal<-Datalong[Datalong$PE_level=="0.2" |
                     (Datalong$PE_level=="0.67" & Datalong$countTR<=48)
                     |
                     Datalong$PE_level=="0.8",]

countTrials<-DatalongBal %>%
  group_by(PE_level) %>%
  tally()

modelmaximalBal<-glmer(id_acc~PE_level*session+(1+session*PE_level|participant)+(1|rec_Obj), 
                    family=binomial(), data= DatalongBal,
                    control=glmerControl(optimizer="bobyqa",optCtrl=list(maxfun=100000)))


summary(modelmaximalBal)

# frop the random slope for PE
modeldropPE<-glmer(id_acc~PE_level*session+(PElevel*session|participant)+(1|rec_Obj), 
                    family=binomial(), data= Datalong,
                    control=glmerControl(optimizer="bobyqa",optCtrl=list(maxfun=100000)))

summary(modeldropPE)

Anova(modeldropPE)

# set controasts
Datalong$PE_level<-as.factor(Datalong$PE_level)
Datalong$session<-as.factor(Datalong$session)
# test for a quadratic trend: u-saped/ inverted u shaped
contrast1<-c(1,-2,1)
# linear
contrast2<-c(-1,0,1)

contrasts(Datalong$PE_level)<-cbind(contrast1, contrast2)

modeldropPE<-glmer(id_acc~PE_level*session+(session|participant)+(1|rec_Obj), 
                   family=binomial(), data= Datalong,
                   control=glmerControl(optimizer="bobyqa",optCtrl=list(maxfun=100000)))

summary(modeldropPE)
Anova(modeldropPE, type="III")


# let's check contr.poly function, with three levels
contrasts(Datalong$PE_level)<-contr.poly(3)

modeldropPE<-glmer(id_acc~PE_level*session+(session|participant)+(1|rec_Obj), 
                   family=binomial(), data= Datalong,
                   control=glmerControl(optimizer="bobyqa",optCtrl=list(maxfun=100000)))

summary(modeldropPE)

# similar
library(car)

Anova(modeldropPE)




################################# trial number encoding
Datatrial<-Datalong %>%
          group_by(enc_trialN) %>%
          summarise(recog = mean(id_acc, na.rm=T))
           

plot(Datatrial$recog, type="l")

# autocorrelation
a<-acf(Datatrial$recog)

# test for time -dependence
timedep<-glmer(id_acc~enc_trialN+(1|participant), family= binomial, data=Datalong)

summary(timedep)

# check for autocorrelation
library(dynlm)
library(car)
library(AER)
a<-dynlm(recog ~ enc_trialN + L(enc_trialN), data = Datatrial)

durbinWatsonTest(a)

durbinWatsonTest(a, max.lag=4)


# at retrieval?
Datatrialret<-Datalong %>%
  group_by(rec_trialN) %>%
  summarise(recog = mean(id_acc, na.rm=T))

# test for time -dependence
timedep<-glmer(id_acc~rec_trialN+(1+rec_trialN|participant), family= binomial, data=Datalong)

summary(timedep)

b<-dynlm(recog ~ rec_trialN + L(rec_trialN), data = Datatrialret)
durbinWatsonTest(b, max.lag=4)

plot(Datatrialret$recog, type="l")

# add it to the model
modeldropPE<-glmer(id_acc~PE_level+rec_trialN+(rec_trialN|participant)+(1|rec_Obj), 
                   family=binomial(), data= Datalong[Datalong$session==1,],
                   control=glmerControl(optimizer="bobyqa",optCtrl=list(maxfun=100000)))

summary(modeldropPE)

############################ 
############### nlme
###########################

# try the maximal random model with lme
# try nlme: log transform the reposne variable (https://stats.stackexchange.com/questions/398869/r-how-to-fit-a-glmm-in-nlme)
Datalong$id_acc.logit<-logit(Datalong$id_acc)

# run lme
detach("package:lme4", unload = TRUE)
library(nlme)

test.lme<-lme(id_acc.logit~PE_level*session, 
              random = ~ (PE_level*session+PE_level:session) | participant, 
              data = Datalong, 
              method = "REML", 
              control= lmeControl( opt = "optim", optimMethod ="BFGS" ))

summary(test.lme)

Anova(test.lme)

# bayesian
library(MCMCglmm)

# create a dataset without missing data
Datalong_nomiss<-Datalong[complete.cases(Datalong$id_acc),]
# it is the same, so keep using the previous one. 

contrasts(Datalong_nomiss$PE_level)<-contr.poly(3)


Datalong$PE_level<-as.factor(Datalong$PE_level)

bayesianinteract<-MCMCglmm(id_acc~PE_level,
                             random = ~idh(PE_level):participant + rec_Obj,
                           #family = "categorical",
                           data=Datalong_nomiss)
                           #nitt=3000, thin=10, burnin=500)
 
summary(bayesianinteract)

Anova
plot(bayesianinteract)

# setting priors
# variance, very large for uninformative priors
varianceint<-10
variancePElev<-10
variancesession<-10
varianceinteract<-10

cov<-0

# create the matrix
var<-matrix(c(varianceint, cov, cov,cov, 
       cov, variancePElev, cov, cov,  
       cov, cov, variancesession, cov, 
       cov, cov, cov, varianceinteract), nrow=4, ncol=4)


# now the means
priormodelinter<-list(B = list(mu=c(0, 0,0, 0 ), # coefficients, centered to 0
                     V = var) )#,
                     # G = list (G1 = list(V = 1, nu =0.002),
                     #           G2 = list(V = 1, nu =0.002),
                     #           G3 = list(V = 1, nu =0.002),
                     #           G4 = list(V = 1, nu =0.002),
                     #           G5 = list(V = 1, nu =0.002)
                     #           ), R = list(V = 1, nu =0.002)) # G is the G structure, the prior distribution for the random effect. V reflect the inverse0 Wishart distribution.
                     # n is the confidence we have in that parameter. here is very low


bayesianinteract<-MCMCglmm(id_acc~PE_level*session,
                           random = ~idh(session*PE_level):participant + rec_Obj, 
                           family = "categorical",
                           data=Datalong,
                           #prior = priormodelinter,
                           nitt=5000, thin=10, burnin=100)

plot(bayesianinteract)

summary(bayesianinteract)


##########################################################
#### brms  
##########################################################
options(timeout= 4000000)

# turn PE_level to numeric
Datalong$PE_level<-as.integer(as.factor(Datalong$PE_level))

# specify priors

prior2 <- c(
  prior(normal(0, 10), class = Intercept), # prior distribution for the intercept
  prior(normal(0, 10), class = b, coef = session), # weakly informative prior distribution for the effect of session
  prior(normal(0, 10), class = b, coef = PE_level), # weakly informative prior distribution for the effect of session
  prior(normal(0, 10), class = b, coef = PE_level:session), # interaction
  prior(cauchy(0,10), class = sd) # standard deviation of the by-subjects intercepts
  
)


fit_b<-brm(id_acc~PE_level +(1+PE_level*session|participant),   #similar to lmwr
           family = bernoulli(link="logit"),
           warmup = 500, 
           iter = 2000, 
           chains = 2, 
           #prior = prior2,
           inits = "0", 
           cores=1,
           data=Datalong)

stanplot(fit_b, 
         type = "areas",
         prob = 0.95)

plot(fit_b)

summary(fit_b)

Datalong$session

setwd(cd)
