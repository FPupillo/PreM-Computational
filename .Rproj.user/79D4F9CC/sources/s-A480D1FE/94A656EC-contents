# Behavioural analysis script
rm(list=ls())

cd<-getwd()


library(ggplot2)
library(dplyr)
library(psycho)
library(lme4)
library(sjstats)
library(brms)

source("helper_functions/discCalc.R")


setwd("premup-priors")

setup<-"priors"

# in this dataset, there are 313 old items. by 30 participants, there are 9360 trials. 

Datalong<-read.csv2("outputs/group_level/share/group_task-rec.csv",sep=";",
                     header=T,stringsAsFactors=FALSE)

names(Datalong)[3]<-"participant"

# create a new variable for the trial type to fit the name in the formula
Datalong$rec_trialType<-Datalong$rec_trial_type

# calculate discriminability by participant
# first, create wide dataset
Datawide<-discCalc(Datalong)

# histogram dprim
hist(Datawide$dprime)

# participants who got less than 1+2SD
outcut<-mean(Datawide$dprime)

excl<-Datawide$SubNum[Datawide$dprime<outcut-2* sd(Datawide$dprime)]

# save it to the exclusion file
# read the exclusion file first
exclusions<-read.csv("computational_model/part_excl.csv")
exclusions[exclusions$setup==setup,]$Part<-excl
# write the exclusion file
write.csv(exclusions,"computational_model/part_excl.csv", row.names = F )

# select only old trials
Datalong<-Datalong[(Datalong$OvsN==1),]

# exclude fillers
#Datalong<-Datalong[Datalong$fillers==0,]

# count by trials
countTrials<-Datalong %>%
  group_by(PE_level) %>%
  tally()

# divide by participants
countTrials$n<-countTrials$n/length(unique(Datalong$participant))

countTrials

# exclude participant
Datalong<-Datalong[!Datalong$participant %in% excl, ]
      
# aggregate data by session, condition, participants
data_agg<-Datalong %>%
  group_by(session, PE_level, participant) %>%
  dplyr::summarise(id_acc = mean(id_acc, na.rm = T))

# congert the factors variables
data_agg$PE_level<-as.factor(data_agg$PE_level)
data_agg$session<-as.factor(data_agg$session)

# rename the levels
levels(data_agg$PE_level)<-c("LowPE", "LowMedPE", "LowHighPE", "HighPE")
levels(data_agg$session)<-c("ImmediateRec", "DelayedRec")

# get within participant SE
library(Rmisc)
dat_summary <- summarySEwithin(data_agg,
                                measurevar = "id_acc",
                                withinvars = c("PE_level", "session"), 
                                idvar = "participant")




ggplot(data_agg, aes(PE_level, id_acc))+ 
  geom_bar(aes(PE_level, id_acc, fill = PE_level),
       position="dodge",stat="summary", fun.y="mean")+
 
  geom_errorbar(aes(y = id_acc, ymin = id_acc - ci, ymax = id_acc + ci),
              color = "black", width = 0.10, data=dat_summary)+
  #geom_boxplot(alpha =0.1)+
  geom_jitter( size=0.4,width=0.1, data=data_agg)+
  theme_bw()+
  theme(axis.text.x = element_blank())+ # we are showing the different levels through the colors, so we can avoid naming the bars
  theme(axis.ticks.x = element_blank())+
  facet_grid(.~session)+
  theme(strip.text.x = element_text(size = 13))+ 
  ylim(c(0,1))+
  ylab("% recognition")

# unconditional model first to check the intraclass correlation
unconditionalM<-glmer(id_acc~(1|participant), family=binomial(), data= Datalong)
summary(unconditionalM)

# intraclass correlation
ic1<-performance::icc(unconditionalM)
# 15% of the variance in recognition memory is between participants

# does adding random intercepts for images improve the fit?
unconditionalMandIM<-glmer(id_acc~(1|participant)+(1|rec_Obj), family=binomial(), data= Datalong)
summary(unconditionalMandIM)

anova(unconditionalM, unconditionalMandIM)
# significant improvement

# does adding random slopes for PE improves it?
# first, convert PE level into factor
Datalong$PE_level<-as.factor(Datalong$PE_level)
unconditionalMimandSesPE<-glmer(id_acc~(1+PE_level|participant)+(1|rec_Obj), family=binomial(), data= Datalong,
                                control=glmerControl(optimizer="bobyqa",optCtrl=list(maxfun=100000)))
# singular fit

anova(unconditionalMandIM, unconditionalMimandSesPE)
# does not improve the fit



# now add the fixed effect and the maximal random structure
modelmaximal<-glmer(id_acc~PE_level*session+(1+session|participant)+(1|rec_Obj), 
                    family=binomial(), data= Datalong,
                    control=glmerControl(optimizer="bobyqa",optCtrl=list(maxfun=100000)))

summary(modelmaximal)

Anova(modelmaxima, type = )
# frop the random slope for PE
modeldropPE<-glmer(id_acc~PE_level*session+(session|participant)+(1|rec_Obj), 
                    family=binomial(), data= Datalong,
                    control=glmerControl(optimizer="bobyqa",optCtrl=list(maxfun=100000)))

summary(modeldropPE)

Anova(modeldropPE)

# set controasts
Datalong$PE_level<-as.factor(Datalong$PE_level)
Datalong$session<-as.factor(Datalong$session)
# test for a quadratic trend: u-saped/ inverted u shaped
contrast1<-c(1,-2,1)
# linear
contrast2<-c(-1,0,1)

contrasts(Datalong$PE_level)<-cbind(contrast1, contrast2)

modeldropPE<-glmer(id_acc~PE_level*session+(PElevel*session|participant)+(1|rec_Obj), 
                   family=binomial(), data= Datalong,
                   control=glmerControl(optimizer="bobyqa",optCtrl=list(maxfun=100000)))

summary(modeldropPE)

# let's check contr.poly function, with three levels
contrasts(Datalong$PE_level)<-contr.poly(4)

modelmax<-glmer(id_acc~PE_level*session+(PE_level*session|participant)+(1|obj_file), 
                   family=binomial(), data= Datalong,
                   control=glmerControl(optimizer="bobyqa",optCtrl=list(maxfun=100000)))

summary(modelmax)

# similar
library(car)

Anova(modelmax, type="III")

# bayesian
library(MCMCglmm)

# create a dataset without missing data
Datalong_nomiss<-Datalong[complete.cases(Datalong$id_acc),]
# it is the same, so keep using the previous one. 

contrasts(Datalong_nomiss$PE_level)<-contr.poly(3)


Datalong$PE_level<-as.factor(Datalong$PE_level)

bayesianinteract<-MCMCglmm(id_acc~PE_level,
                             random = ~idh(PE_level):participant + rec_Obj,
                           #family = "categorical",
                           data=Datalong_nomiss)
                           #nitt=3000, thin=10, burnin=500)
 
summary(bayesianinteract)

Anova
plot(bayesianinteract)

# setting priors
# variance, very large for uninformative priors
varianceint<-10
variancePElev<-10
variancesession<-10
varianceinteract<-10

cov<-0

# create the matrix
var<-matrix(c(varianceint, cov, cov,cov, 
       cov, variancePElev, cov, cov,  
       cov, cov, variancesession, cov, 
       cov, cov, cov, varianceinteract), nrow=4, ncol=4)


# now the means
priormodelinter<-list(B = list(mu=c(0, 0,0, 0 ), # coefficients, centered to 0
                     V = var) )#,
                     # G = list (G1 = list(V = 1, nu =0.002),
                     #           G2 = list(V = 1, nu =0.002),
                     #           G3 = list(V = 1, nu =0.002),
                     #           G4 = list(V = 1, nu =0.002),
                     #           G5 = list(V = 1, nu =0.002)
                     #           ), R = list(V = 1, nu =0.002)) # G is the G structure, the prior distribution for the random effect. V reflect the inverse0 Wishart distribution.
                     # n is the confidence we have in that parameter. here is very low


bayesianinteract<-MCMCglmm(id_acc~PE_level*session,
                           random = ~idh(session*PE_level):participant + rec_Obj, 
                           family = "categorical",
                           data=Datalong,
                           #prior = priormodelinter,
                           nitt=5000, thin=10, burnin=100)

plot(bayesianinteract)

summary(bayesianinteract)


##########################################################
#### brms  
##########################################################
options(timeout= 4000000)

# turn PE_level to numeric
Datalong$PE_level<-as.integer(as.factor(Datalong$PE_level))

# specify priors

prior2 <- c(
  prior(normal(0, 10), class = Intercept), # prior distribution for the intercept
  prior(normal(0, 10), class = b, coef = session), # weakly informative prior distribution for the effect of session
  prior(normal(0, 10), class = b, coef = PE_level), # weakly informative prior distribution for the effect of session
  prior(normal(0, 10), class = b, coef = PE_level:session), # interaction
  prior(cauchy(0,10), class = sd) # standard deviation of the by-subjects intercepts
  
)


fit_b<-brm(id_acc~1+(1|PE_level) +(1+PE_level|participant),   #similar to lmwr
           family = bernoulli(link="logit"),
           warmup = 500, 
           iter = 2000, 
           chains = 2, 
           #prior = prior2,
           inits = "0", 
           cores=1,
           data=Datalong)

stanplot(fit_b, 
         type = "areas",
         prob = 0.95)

plot(fit_b)

summary(fit_b)

Datalong$session
